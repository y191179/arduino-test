{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y191179/arduino-test/blob/main/datamodels_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqowJU6UbWgq"
      },
      "outputs": [],
      "source": [
        "# imports & functions\n",
        "import torch as ch\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "import gdown\n",
        "\n",
        "def load_datamodels(file_path: str):\n",
        "    # Load the saved file\n",
        "    data = ch.load(file_path,  weights_only=True)\n",
        "\n",
        "    # Access the contents\n",
        "    weight = data['weight']\n",
        "    bias = data['bias']\n",
        "    lam = data['lam']\n",
        "\n",
        "    return weight, bias, lam\n",
        "\n",
        "\n",
        "def get_binary_labels(dataset, animate_labels: set = {2, 3, 4, 5, 6, 7}) -> List[bool]:\n",
        "    binary_targets = [label in animate_labels for label in dataset.targets]\n",
        "    return binary_targets"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGhdc9nAoIpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment**\n",
        "\n",
        "In this experiment, we trained circa 60,000 classifiers on random subsets of the CIFAR10 training set (alpha = 0.1) on the task to predict animate vs inanimate objects. This is a comparatively simple task compared to learning to predict all 10 classes (dogs, airplanes, e.t.c ...) and allows us to investigate model class behavior with some information on known sub-classes in the dataset for which models will likely have learnt different patterns (when trained on random subsets of the data) to solve this task.\n",
        "\n",
        "**Why are we using this 'toy' dataset and not biomedical data?**\n",
        "\n",
        "In using this framework to uncover underlying subclasses from biomedical datasets, we will likely not have access to such ground-truth labels. Because we do have these labels for CIFAR10 and because the data are easy to visualize, we can quickly start to explore what features the models may be using to solve a binary classification task with datamodels in this example.\n",
        "\n",
        "**Note**\n",
        "\n",
        "Note that to facilitate speed in training these models, we only used 50% of the CIFAR10 training set from which to sample subsets of training datapoints (since this is a relatively simple task) and our datamodels are computed 'only' on 60,000 models (compare to results from Ilyas et al on the full dataset 10-class classifiers trained on >1 M models: https://arxiv.org/pdf/2202.00622)."
      ],
      "metadata": {
        "id": "aRdIAAKABa9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get datamodels\n",
        "file_id = '1YJDHCAI56AXJ6fv3PSRucyFkskcWPxqt'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "gdown.download(url, 'datamodels.pt', quiet=False)\n",
        "\n",
        "# get datamodels outputs\n",
        "datamodels =load_datamodels('datamodels.pt')\n",
        "weights = datamodels[0].cpu().numpy()\n",
        "bias = datamodels[1].cpu().numpy()\n",
        "\n",
        "# load the CIFAR10 dataset\n",
        "ds_test = torchvision.datasets.CIFAR10('.', train=False, download=True)\n",
        "ds_train = torchvision.datasets.CIFAR10('.', train=True, download=True)\n"
      ],
      "metadata": {
        "id": "9_-mFtusuLNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "841070a0-5dd7-4397-b4b2-52cc38b9755c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1YJDHCAI56AXJ6fv3PSRucyFkskcWPxqt\n",
            "From (redirected): https://drive.google.com/uc?id=1YJDHCAI56AXJ6fv3PSRucyFkskcWPxqt&confirm=t&uuid=dcc668ce-e19b-43fc-9696-294aaab89f5d\n",
            "To: /content/datamodels.pt\n",
            "100%|██████████| 1.00G/1.00G [00:09<00:00, 104MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 47451447.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First, we'll have a look at a lower dimensional projection of the datamodels embedding.**\n",
        "\n",
        "The datamodels embedding of a given input (here the test CIFAR10 dataset on which we evaluated model performance trained on different subsets of the train dataset) into $\\mathbb{R}^d$, where $d$ is the size of the training set (here 25,000). We'll compare the cumulative variance explained by the first N prinicpal components for projections of datamodel embeddings to projections of an equal number of randomly sampled pixels from the original dataset.\n",
        "\n",
        "- *What do you notice?*"
      ],
      "metadata": {
        "id": "HOzpQRSVIaJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Npcs = 500\n",
        "\n",
        "cifar_data = np.array(ds_test.data)\n",
        "# Pick the same random pixels for each sample\n",
        "random_pixel_indices = np.random.choice(cifar_data.shape[1] * cifar_data.shape[2] * cifar_data.shape[3], 1000, replace=False)\n",
        "# Reshape the data to be a 2D array where each row is a sample and each column is a pixel\n",
        "cifar_data_reshaped = cifar_data.reshape(cifar_data.shape[0], -1)\n",
        "# Select the random pixels for each sample\n",
        "selected_pixels = cifar_data_reshaped[:, random_pixel_indices] # 1000 random pixels for 10000 samples in the test set\n",
        "# compute projections\n",
        "pca = PCA(n_components=Npcs)\n",
        "pca_result = pca.fit_transform(selected_pixels)\n",
        "# Plot the cumulative explained variance ratio of the principal components\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "plt.figure(figsize=(4, 3))\n",
        "plt.plot(range(0, Npcs), cumulative_variance, marker='o', linestyle='--', alpha=0.7, markersize=1, label='projection from pixel')\n",
        "\n",
        "\n",
        "# Perform PCA on the weights\n",
        "pca = PCA(n_components=Npcs)\n",
        "temp = weights.T[:,:1000] # 1000 datamodel embeddings for 10000 samples in the test set\n",
        "x = (temp - np.mean(temp, axis=0)) / np.std(temp, axis=0)\n",
        "pca_result = pca.fit_transform(temp)\n",
        "# Plot the cumulative explained variance ratio of the principal components\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "plt.plot(range(0, Npcs), cumulative_variance, marker='o', linestyle='--', alpha=0.7, markersize=1, label='projection from datamodels embeddings')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PLeI0g55EbnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The datamodel embeddings seem to spread out the variance over a high number of dimensions,leading to a high effective dimensionality.\n",
        "\n",
        "**Now we'll perform PCA on the full datamodels embedding and have a look at the principal components.**\n",
        "\n",
        "This should start to give you an intution for what features the models may be learning to solve the classification task, for different subsets of the population.\n",
        "\n",
        "- *What do you notice when you look at the first few PCs computed on all the test samples?*\n",
        "\n",
        "- *What do you notice when we look at the first few PCs computed on only one of the classes we trained on (inanimate vs animate)?*"
      ],
      "metadata": {
        "id": "zXumzzUdKf3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=20)\n",
        "temp = weights.T\n",
        "x = (temp - np.mean(temp, axis=0)) / np.std(temp, axis=0)\n",
        "pca_result_all = pca.fit_transform(x)\n"
      ],
      "metadata": {
        "id": "bhOVA1oNOqVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# project the full test data\n",
        "PC = 0 # explore different PCs\n",
        "\n",
        "indices_sorted = np.argsort(pca_result_all[:,PC])\n",
        "\n",
        "fig, axes = plt.subplots(3, 10, figsize=(10, 3))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(ds_test.data[indices_sorted][i])\n",
        "    ax.axis('off')\n",
        "fig.suptitle(f'Negative extreme of PC {PC}', fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(3, 10, figsize=(10, 3))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(ds_test.data[indices_sorted][-(i+1)])\n",
        "    ax.axis('off')\n",
        "fig.suptitle(f'Positive extreme of PC {PC}', fontsize=16)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "jM0CzsS5Ovy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZp65wfpbWgs"
      },
      "outputs": [],
      "source": [
        "get_animate=False #### Set True or False for the test class you want to compute PC projections for ####\n",
        "\n",
        "if get_animate:\n",
        "  pca = PCA(n_components=20)\n",
        "  indices = np.array(get_binary_labels(ds_test))\n",
        "  temp = weights.T[indices]\n",
        "  x = (temp - np.mean(temp, axis=0)) / np.std(temp, axis=0)\n",
        "  pca_result = pca.fit_transform(x)\n",
        "  subset = ds_test.data[indices]\n",
        "else:\n",
        "  pca = PCA(n_components=20)\n",
        "  indices = np.invert(get_binary_labels(ds_test))\n",
        "  temp = weights.T[indices]\n",
        "  x = (temp - np.mean(temp, axis=0)) / np.std(temp, axis=0)\n",
        "  pca_result = pca.fit_transform(x)\n",
        "  subset = ds_test.data[indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0xfn31jbWgs"
      },
      "outputs": [],
      "source": [
        "PC = 2 #### Have a look at a few PCs ####\n",
        "\n",
        "indices_sorted = np.argsort(pca_result[:,PC])\n",
        "\n",
        "fig, axes = plt.subplots(3, 10, figsize=(10, 3))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(subset[indices_sorted][i])\n",
        "    ax.axis('off')\n",
        "fig.suptitle(f'Negative extreme of PC {PC}', fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(3, 10, figsize=(10, 3))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(subset[indices_sorted][-(i+1)])\n",
        "    ax.axis('off')\n",
        "fig.suptitle(f'Positive extreme of PC {PC}', fontsize=16)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let's have a look at the top influencing training images for a given test set image**\n",
        "\n",
        "(i.e. the training images with the largest positive weights in the datamodels matrix for a given test image).\n",
        "\n",
        "- *What about the top negative influencers (i.e. images that make the model confident in an incorrect prediction)? What do you expect these images to look like?*\n",
        "\n",
        "- *What about train images that don't seem to influence the models predictions?*"
      ],
      "metadata": {
        "id": "WRMX9iPCPdPt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELXHbE97bWgs"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=10, figsize=(20, 4))  # 2 rows, 10 columns, adjust figure size\n",
        "\n",
        "for test_id in range(10):\n",
        "    # Test Image in the first row (first row, test_id-th column)\n",
        "    axes[0, test_id].imshow(ds_test.data[test_id])  # Plot test image\n",
        "    axes[0, test_id].set_title('Test Image')  # Set title for the test image\n",
        "    axes[0, test_id].axis('off')  # Turn off axis\n",
        "\n",
        "    # Top Influencer in the second row (second row, test_id-th column)\n",
        "    influencer_id = np.argmax(weights[:, test_id])  # Get the index of the top influencer\n",
        "    axes[1, test_id].imshow(ds_train.data[influencer_id])  # Plot top influencer image\n",
        "    axes[1, test_id].set_title('Top Influencer')  # Set title for the top influencer\n",
        "    axes[1, test_id].axis('off')  # Turn off axis\n",
        "\n",
        "# Adjust layout to prevent overlap and improve spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- *What happens if we try to cluster on the datamodels embeddings? (We'll just use the top PCs to make this faster).\n",
        "What do you notice?*\n"
      ],
      "metadata": {
        "id": "HlYzd3bJSKnI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h95jKeJFbWgs"
      },
      "outputs": [],
      "source": [
        "# Perform KMeans Clustering on the PCA result\n",
        "kmeans = KMeans(n_clusters=10, random_state=0)\n",
        "kmeans_result = kmeans.fit_predict(pca_result_all)\n",
        "\n",
        "for i in range(4):\n",
        "    fig, axes = plt.subplots(6, 15, figsize=(15, 5))\n",
        "    indices = np.argwhere(kmeans_result==i).reshape(-1)\n",
        "    for x, ax in enumerate(axes.flat):\n",
        "        ax.imshow(ds_test.data[indices[x]])\n",
        "        ax.axis('off')\n",
        "    fig.suptitle(f'Kmeans cluster {i}', fontsize=16)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finally, we'll run a simple experiment to understand how the datamodels embeddings may be useful to new tasks.**\n",
        "\n",
        "Instead of using the images as features to train some classifier, we can now use our datamodel weights (i.e. the influencers) as embeddings (i.e. a representation of our data), which - as we've seen - are much more interpretable than the pixels we started from.\n",
        "\n",
        "As an illustration of this, let's do the following:\n",
        "\n",
        "- In the first instance, we'll train a linear classifier on a random sample of pixels from the CIFAR10 test data as features. We'll split these data into train and validation sets and fit a linear classifier to predict a single subclass from these data (see below for available subclasses).\n",
        "\n",
        "- In the second instance, we'll train a linear classifier on a subset of datamodel embeddings. We'll again split these data into train and validation sets and fit a linear classifier to predict a single subclass from these data.\n",
        "\n",
        "Try varying the numbers of features, different target subclasses, and training size.\n",
        "\n",
        "- *What do you notice as you increase the number of features?*\n",
        "\n",
        "- *What do you notice when you use all available features? And what if you also dramatically decrease the training size (e.g. set test_size to 0.95)?*\n",
        "\n",
        "- *Instead of using the datamodel features, you use the PCAs that summarize the influencers, how many top PCs are sufficient to solve this task?*\n"
      ],
      "metadata": {
        "id": "bfwtSZKhFpBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_test.class_to_idx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l86TPttUJVe",
        "outputId": "d05637b7-cddd-4e2d-efa0-88fb651ee669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'airplane': 0,\n",
              " 'automobile': 1,\n",
              " 'bird': 2,\n",
              " 'cat': 3,\n",
              " 'deer': 4,\n",
              " 'dog': 5,\n",
              " 'frog': 6,\n",
              " 'horse': 7,\n",
              " 'ship': 8,\n",
              " 'truck': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example: try 100, 1000, 3000, and 'All'\n",
        "nfeatures = 0\n",
        "label = 0\n",
        "test_size = 0.2 # also try 0.9\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "n5h6oCTzU0TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the entire CIFAR10 dataset\n",
        "cifar_data = np.array(ds_test.data)\n",
        "target = np.array(ds_test.targets)==label ### Try different sublclasses ###\n",
        "# Reshape the data to be a 2D array where each row is a sample and each column is a pixel\n",
        "cifar_data_reshaped = cifar_data.reshape(cifar_data.shape[0], -1)\n",
        "\n",
        "if nfeatures=='All':\n",
        "  print('using all features')\n",
        "  features = cifar_data_reshaped\n",
        "else:\n",
        "  print(f'using {nfeatures} features')\n",
        "  # Pick the same random pixels for each sample\n",
        "  np.random.seed(seed)\n",
        "  random_pixel_indices = np.random.choice(cifar_data.shape[1] * cifar_data.shape[2] * cifar_data.shape[3], nfeatures, replace=False) ### Try different numbers of pixels (max is 3072) ###\n",
        "\n",
        "  # Select the same random pixels for each sample\n",
        "  features = cifar_data_reshaped[:, random_pixel_indices]\n",
        "\n",
        "# Balance the dataset by undersampling the majority class\n",
        "rus = RandomUnderSampler(random_state=seed)\n",
        "features_resampled, target_resampled = rus.fit_resample(features, target)\n",
        "\n",
        "# Split the balanced dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_resampled, target_resampled, test_size=test_size, random_state=seed) ### Try out different train sizes ###\n",
        "\n",
        "# Initialize and train the linear classifier\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Compute the validation accuracy\n",
        "validation_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Validation Accuracy: {validation_accuracy:.4f}')"
      ],
      "metadata": {
        "id": "qEjrZycTTb8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = np.array(ds_test.targets)==label\n",
        "\n",
        "if nfeatures=='All':\n",
        "  print('using all features')\n",
        "  features = weights.T\n",
        "else:\n",
        "  print(f'using {nfeatures} features')\n",
        "  features = weights.T[:,:nfeatures]\n",
        "\n",
        "# Balance the dataset by undersampling the majority class\n",
        "rus = RandomUnderSampler(random_state=seed)\n",
        "features_resampled, target_resampled = rus.fit_resample(features, target)\n",
        "\n",
        "# Split the balanced dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_resampled, target_resampled, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Initialize and train the linear classifier\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Compute the validation accuracy\n",
        "validation_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Validation Accuracy: {validation_accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "k8-IbloATA7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **This example illustrates that we can now learn the simplest model possible - a linear classifier - by using the datamodel embeddings instead of the pixel values and obtain very good and generalizable performance on a task that we never explicitly had labels for when we trained our datamodels.**"
      ],
      "metadata": {
        "id": "zD-QexxxboKb"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}